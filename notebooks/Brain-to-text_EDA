{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12491966,"sourceType":"datasetVersion","datasetId":7875034}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-24T08:54:21.468437Z","iopub.execute_input":"2025-07-24T08:54:21.468779Z","iopub.status.idle":"2025-07-24T08:54:23.424327Z","shell.execute_reply.started":"2025-07-24T08:54:21.468754Z","shell.execute_reply":"2025-07-24T08:54:23.423435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport h5py\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport os\n\n# --- Define the base directory ---\nBASE_DIR = '/kaggle/input/brain-to-text-25-data/t15_copyTask_neuralData/hdf5_data_final'\n\nNEURAL_DATA_KEY = 'input_features'\nTRANSCRIPTION_KEY = 'transcription'\n\ndef load_metadata_from_hdf5(file_path):\n    \"\"\"\n    Correctly loads metadata based on the now-known HDF5 structure.\n    \"\"\"\n    metadata = []\n    try:\n        with h5py.File(file_path, 'r') as f:\n            # The top-level keys ARE the trials.\n            for trial_key in f.keys():\n                trial_group = f[trial_key]\n                \n                # Check if the group contains the correct dataset names\n                if isinstance(trial_group, h5py.Group) and NEURAL_DATA_KEY in trial_group and TRANSCRIPTION_KEY in trial_group:\n                    \n                    num_time_bins = trial_group[NEURAL_DATA_KEY].shape[0]\n                    \n                    # The transcription is an array of integers, not a string.\n                    # We will load it as a list of numbers for now.\n                    transcription_ids = list(trial_group[TRANSCRIPTION_KEY][()])\n                    \n                    metadata.append({\n                        'trial_id': trial_key,\n                        'num_time_bins': num_time_bins,\n                        'transcription_ids': transcription_ids,\n                        # We can't get num_words directly yet, so we'll estimate from the length of the ID list.\n                        # This might not be perfect but is a good start.\n                        'num_words_estimate': len(transcription_ids) \n                    })\n    except Exception as e:\n        print(f\"Error processing file {file_path}: {e}\")\n        import traceback\n        traceback.print_exc()\n        \n    return metadata\n\ndef load_test_metadata_from_hdf5(file_path):\n    \"\"\" Loads test data which only has input_features. \"\"\"\n    metadata = []\n    try:\n        with h5py.File(file_path, 'r') as f:\n            for trial_key in f.keys():\n                trial_group = f[trial_key]\n                if isinstance(trial_group, h5py.Group) and NEURAL_DATA_KEY in trial_group:\n                    num_time_bins = trial_group[NEURAL_DATA_KEY].shape[0]\n                    metadata.append({\n                        'trial_id': trial_key,\n                        'num_time_bins': num_time_bins\n                    })\n    except Exception as e:\n        print(f\"Error processing file {file_path}: {e}\")\n    return metadata\n\n\n# --- Main Loading Loop ---\nall_metadata = []\nsession_dirs = sorted([d for d in os.listdir(BASE_DIR) if os.path.isdir(os.path.join(BASE_DIR, d))])\n\nfor session in tqdm(session_dirs, desc=\"Processing Sessions\"):\n    session_path = os.path.join(BASE_DIR, session)\n    for split in ['train', 'val', 'test']:\n        file_name = f'data_{split}.hdf5'\n        file_path = os.path.join(session_path, file_name)\n        \n        if os.path.exists(file_path):\n            if split == 'test':\n                session_metadata = load_test_metadata_from_hdf5(file_path)\n            else:\n                session_metadata = load_metadata_from_hdf5(file_path)\n            \n            # Add session and split info to the found trials\n            for item in session_metadata:\n                item['session'] = session\n                item['split'] = split\n            all_metadata.extend(session_metadata)\n\ndf = pd.DataFrame(all_metadata)\n\n# --- Final Verification and Display ---\nprint(f\"Loaded a total of {len(df)} trials.\")\nif not df.empty:\n    print(f\"Data splits:\\n{df['split'].value_counts()}\")\n    # We no longer have 'sentence_text', so display the new columns\n    display(df.head())\nelse:\n    print(\"DataFrame is still empty. This indicates a very unusual issue.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T09:38:07.859836Z","iopub.execute_input":"2025-07-24T09:38:07.860158Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T09:40:06.513540Z","iopub.execute_input":"2025-07-24T09:40:06.514529Z","iopub.status.idle":"2025-07-24T09:40:06.534248Z","shell.execute_reply.started":"2025-07-24T09:40:06.514448Z","shell.execute_reply":"2025-07-24T09:40:06.533073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import h5py\nimport sys\n\ndef load_h5py_file(file_path):\n    data = {\n        'neural_features': [],\n        'n_time_steps': [],\n        'seq_class_ids': [],\n        'seq_len': [],\n        'transcriptions': [],\n        'sentence_label': [],\n        'session': [],\n        'block_num': [],\n        'trial_num': [],\n    }\n    # Open the hdf5 file for that day\n    with h5py.File(file_path, 'r') as f:\n\n        keys = list(f.keys())\n\n        # For each trial in the selected trials in that day\n        for key in keys:\n            g = f[key]\n\n            neural_features = g['input_features'][:]\n            n_time_steps = g.attrs['n_time_steps']\n            seq_class_ids = g['seq_class_ids'][:] if 'seq_class_ids' in g else None\n            seq_len = g.attrs['seq_len'] if 'seq_len' in g.attrs else None\n            transcription = g['transcription'][:] if 'transcription' in g else None\n            sentence_label = g.attrs['sentence_label'][:] if 'sentence_label' in g.attrs else None\n            session = g.attrs['session']\n            block_num = g.attrs['block_num']\n            trial_num = g.attrs['trial_num']\n\n            data['neural_features'].append(neural_features)\n            data['n_time_steps'].append(n_time_steps)\n            data['seq_class_ids'].append(seq_class_ids)\n            data['seq_len'].append(seq_len)\n            data['transcriptions'].append(transcription)\n            data['sentence_label'].append(sentence_label)\n            data['session'].append(session)\n            data['block_num'].append(block_num)\n            data['trial_num'].append(trial_num)\n    return data\n\n#Generate all referencable file names in the main dataset, split into train, test, and val.\ndef generate_file_names(BASE_DIR):\n    file_names_train = []\n    file_names_test = []\n    file_names_val = []\n    for folder in os.listdir(BASE_DIR):\n        folder_name = BASE_DIR + '/' + folder\n        for file in os.listdir(folder_name):\n            file_name = BASE_DIR + '/' + folder + '/' + file\n            if file == 'data_train.hdf5':\n                file_names_train.append(file_name)\n            elif file == 'data_test.hdf5':\n                file_names_test.append(file_name)\n            elif file == 'data_val.hdf5':\n                file_names_val.append(file_name)\n            else:\n                raise Exception('Unrecognized file name')\n            \n\n    return file_names_train, file_names_test, file_names_val\n\n\nfile_names_train, file_names_test, file_names_val = generate_file_names(BASE_DIR)\n\nfor file_name in file_names_train:\n    data = load_h5py_file(file_path)\n\n    for key, vals in data.items():\n        print(vals)\n        sys.exit()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T10:06:17.721765Z","iopub.execute_input":"2025-07-24T10:06:17.722117Z","iopub.status.idle":"2025-07-24T10:06:22.230984Z","shell.execute_reply.started":"2025-07-24T10:06:17.722093Z","shell.execute_reply":"2025-07-24T10:06:22.229555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(os.listdir(BASE_DIR))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T09:54:51.105042Z","iopub.execute_input":"2025-07-24T09:54:51.105346Z","iopub.status.idle":"2025-07-24T09:54:51.116470Z","shell.execute_reply.started":"2025-07-24T09:54:51.105325Z","shell.execute_reply":"2025-07-24T09:54:51.115318Z"}},"outputs":[],"execution_count":null}]}