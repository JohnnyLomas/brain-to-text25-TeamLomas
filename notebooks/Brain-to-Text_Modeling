{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12491966,"sourceType":"datasetVersion","datasetId":7875034}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Modeling for Brain-to-Text '25","metadata":{}},{"cell_type":"markdown","source":"## Model Definition\n\nWe use a PatchTST encoder paired with the FLAN-T5 decoder to effectively model multivariate timeseries data and convert it to text.","metadata":{}},{"cell_type":"code","source":"from transformers import PatchTSTConfig, PatchTSTModel, AutoModelForSeq2SeqLM, AutoTokenizer\nconfiguration = PatchTSTConfig(prediction_length=12)\n\n# Randomly initializing a model (with random weights) from the configuration\nencoder = PatchTSTModel(configuration)\n\ndecoder = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\nlm_head = decoder.lm_head\ndecoder = decoder.decoder\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\ndecoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T23:55:48.972673Z","iopub.execute_input":"2025-07-24T23:55:48.973180Z","iopub.status.idle":"2025-07-24T23:55:54.102023Z","shell.execute_reply.started":"2025-07-24T23:55:48.973161Z","shell.execute_reply":"2025-07-24T23:55:54.101309Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7192a3d98910426080992f3314113208"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21ba553ca1644f738edeca818b275158"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2812c44f08c84741b8b38b5fe94b0052"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9902134e15f4badb0ed3adb3042ee50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9654989859604a4fb6aafe282fbd604c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"338908ee6990431b91f6809fe183081d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4b7596795ec4c6196a36654af290dfe"}},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 512)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 6)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-7): 7 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 6)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-7): 7 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n)"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"decoder.decoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T23:57:50.579302Z","iopub.execute_input":"2025-07-24T23:57:50.579593Z","iopub.status.idle":"2025-07-24T23:57:50.586538Z","shell.execute_reply.started":"2025-07-24T23:57:50.579574Z","shell.execute_reply":"2025-07-24T23:57:50.585774Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"T5Stack(\n  (embed_tokens): Embedding(32128, 512)\n  (block): ModuleList(\n    (0): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n            (relative_attention_bias): Embedding(32, 6)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseGatedActDense(\n            (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n            (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n            (wo): Linear(in_features=1024, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): NewGELUActivation()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (1-7): 7 x T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=512, out_features=384, bias=False)\n            (k): Linear(in_features=512, out_features=384, bias=False)\n            (v): Linear(in_features=512, out_features=384, bias=False)\n            (o): Linear(in_features=384, out_features=512, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseGatedActDense(\n            (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n            (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n            (wo): Linear(in_features=1024, out_features=512, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n            (act): NewGELUActivation()\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (final_layer_norm): T5LayerNorm()\n  (dropout): Dropout(p=0.1, inplace=False)\n)"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## Dataset Management\n\nCreate a custom dataset class for efficient dataloading during training and testing.","metadata":{}},{"cell_type":"code","source":"import os\nimport h5py\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport torch\nfrom torch.utils.data import Dataset\n\n# Metadata loading function provided by competition host\ndef load_metadata_from_hdf5(file_path):\n    \"\"\"\n    Correctly loads metadata based on the now-known HDF5 structure.\n    \"\"\"\n    metadata = []\n    try:\n        with h5py.File(file_path, 'r') as f:\n            # The top-level keys ARE the trials.\n            for trial_key in f.keys():\n                trial_group = f[trial_key]\n                \n                # Check if the group contains the correct dataset names\n                if isinstance(trial_group, h5py.Group) and NEURAL_DATA_KEY in trial_group and TRANSCRIPTION_KEY in trial_group:\n                    \n                    num_time_bins = trial_group[NEURAL_DATA_KEY].shape[0]\n                    \n                    # The transcription is an array of integers, not a string.\n                    # We will load it as a list of numbers for now.\n                    transcription_ids = list(trial_group[TRANSCRIPTION_KEY][()])\n                    \n                    metadata.append({\n                        'trial_id': trial_key,\n                        'num_time_bins': num_time_bins,\n                        'transcription_ids': transcription_ids,\n                        # We can't get num_words directly yet, so we'll estimate from the length of the ID list.\n                        # This might not be perfect but is a good start.\n                        'num_words_estimate': len(transcription_ids) \n                    })\n    except Exception as e:\n        print(f\"Error processing file {file_path}: {e}\")\n        import traceback\n        traceback.print_exc()\n        \n    return metadata\n\n# Metadata loading function provided by competition host\ndef load_test_metadata_from_hdf5(file_path):\n    \"\"\" Loads test data which only has input_features. \"\"\"\n    metadata = []\n    try:\n        with h5py.File(file_path, 'r') as f:\n            for trial_key in f.keys():\n                trial_group = f[trial_key]\n                if isinstance(trial_group, h5py.Group) and NEURAL_DATA_KEY in trial_group:\n                    num_time_bins = trial_group[NEURAL_DATA_KEY].shape[0]\n                    metadata.append({\n                        'trial_id': trial_key,\n                        'num_time_bins': num_time_bins\n                    })\n    except Exception as e:\n        print(f\"Error processing file {file_path}: {e}\")\n    return metadata\n\ndef decode_transcription(ids: np.ndarray) -> str:\n    \"\"\"\n    Converts a NumPy array of ASCII character IDs to a string, \n    stopping at the first null (0) character.\n    \"\"\"\n    if not isinstance(ids, np.ndarray):\n        ids = np.array(ids, dtype=np.uint8)\n\n    zero_indices = np.where(ids == 0)[0]\n    end = zero_indices[0] if zero_indices.size > 0 else len(ids)\n    return \"\".join(map(chr, ids[:end]))\n\n\nclass b2tDataset(Dataset):\n    \n    '''\n    Provides a dataset for accessing training, validation, and test data.\n    \n    sets: [train|val|test] specifies the split held by the dataset\n    dir:  [str] directory to search for hdf5 data files\n    '''\n    def __init__(self, \n                 sets=\"train\",\n                 dir=\"/kaggle/input/brain-to-text-25-data/t15_copyTask_neuralData/hdf5_data_final/\"\n                ):\n        \n        self.set_type = sets\n        self.files = []\n        self.session_counts = []\n        self.all_metadata = []\n    \n        #Get a list of files for the datasplit\n        for dirname, _, filenames in os.walk(dir):\n            for filename in filenames:\n                if self.set_type in filename:\n                    self.files.append(os.path.join(dirname, filename))\n\n        # Get metadata for the files in the dataset\n        for file_path in tqdm(self.files, desc=\"Processing sessions\"):\n            if os.path.exists(file_path):\n                if self.set_type == 'test':\n                    session_metadata = load_test_metadata_from_hdf5(file_path)\n                else:\n                    session_metadata = load_metadata_from_hdf5(file_path)\n                \n                self.session_counts.append(len(session_metadata))\n                self.all_metadata.extend(session_metadata)       \n    \n    def __len__(self):\n        return sum(self.session_counts)\n\n    def __getitem__(self, idx):\n        total = 0\n        for i,count in enumerate(self.session_counts):\n            total += count\n            if total > idx:\n                break\n        file = self.files[i]\n        index = idx - total + count\n\n        target = None\n        with h5py.File(file, 'r') as f:\n            inputs = torch.from_numpy(f[list(f.keys())[index]]['input_features'][:])\n            if self.set_type != \"test\":\n                target = f[list(f.keys())[index]]['transcription'][:]\n\n        # TODO: Tokenize target data for LLM loss computation\n        target = decode_transcription(target)\n        \n        return (inputs, target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T22:46:38.536339Z","iopub.execute_input":"2025-07-24T22:46:38.537232Z","iopub.status.idle":"2025-07-24T22:46:38.562353Z","shell.execute_reply.started":"2025-07-24T22:46:38.537200Z","shell.execute_reply":"2025-07-24T22:46:38.561484Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"ds = b2tDataset()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T22:46:42.420031Z","iopub.execute_input":"2025-07-24T22:46:42.420339Z","iopub.status.idle":"2025-07-24T22:47:18.074645Z","shell.execute_reply.started":"2025-07-24T22:46:42.420316Z","shell.execute_reply":"2025-07-24T22:47:18.073746Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Processing sessions:   0%|          | 0/45 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d44e469e62a47ba9b6838da7f295b25"}},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"def decode_transcription(ids: np.ndarray) -> str:\n    \"\"\"Converts a NumPy array of ASCII character IDs to a string, stopping at the first null (0) character.\"\"\"\n    if not isinstance(ids, np.ndarray):\n        ids = np.array(ids, dtype=np.uint8)\n\n    zero_indices = np.where(ids == 0)[0]\n    end = zero_indices[0] if zero_indices.size > 0 else len(ids)\n    return \"\".join(map(chr, ids[:end]))\nds.__getitem__(0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T22:48:17.214782Z","iopub.execute_input":"2025-07-24T22:48:17.215182Z","iopub.status.idle":"2025-07-24T22:48:17.302915Z","shell.execute_reply.started":"2025-07-24T22:48:17.215152Z","shell.execute_reply":"2025-07-24T22:48:17.302217Z"}},"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"(tensor([[-0.1082, -0.7717, -0.8542,  ..., -0.7924, -0.8844, -0.6678],\n         [-0.1082, -0.7717, -0.8542,  ...,  0.3434,  0.1390, -0.5005],\n         [-0.1082,  2.4298, -0.8542,  ..., -0.0919, -0.0868, -0.5236],\n         ...,\n         [-0.1082,  0.2954,  0.1956,  ..., -1.3008, -1.2638, -0.7519],\n         [-0.1082, -0.7717, -0.8542,  ..., -0.8352, -0.7602, -0.6557],\n         [-0.1082, -0.7717,  1.2454,  ..., -0.6291, -0.6748, -0.6739]]),\n \"It's not at all anything worth talking about.\")"},"metadata":{}}],"execution_count":50}]}